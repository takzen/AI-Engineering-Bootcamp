# Kompletny Przewodnik Budowy Projektu: RODO Ekspert AI

Witaj w szczegÃ³Å‚owym przewodniku krok po kroku, ktÃ³ry dokumentuje proces tworzenia aplikacji **RODO Ekspert AI**. Ten plik jest kompletnym podsumowaniem moduÅ‚u praktycznego i zawiera wszystkie fragmenty kodu oraz wyjaÅ›nienia niezbÄ™dne do zrozumienia i odtworzenia projektu od zera.

## Spis TreÅ›ci
1. [Cel i Architektura Projektu](#1-cel-i-architektura-projektu)
2. [Krok 1: Struktura Projektu i Konfiguracja Åšrodowiska](#2-krok-1-struktura-projektu-i-konfiguracja-Å›rodowiska)
3. [Krok 2: Proces Ingestii Danych â€“ Tworzenie Bazy Wiedzy](#3-krok-2-proces-ingestii-danych--tworzenie-bazy-wiedzy)
4. [Krok 3: RdzeÅ„ Logiki AI â€“ Budowa ÅaÅ„cucha Q&A](#4-krok-3-rdzeÅ„-logiki-ai--budowa-Å‚aÅ„cucha-qa)
5. [Krok 4: Backend API w FastAPI](#5-krok-4-backend-api-w-fastapi)
6. [Krok 5: Interfejs UÅ¼ytkownika w Streamlit](#6-krok-5-interfejs-uÅ¼ytkownika-w-streamlit)
7. [Krok 6: Uruchomienie Kompletnej Aplikacji](#7-krok-6-uruchomienie-kompletnej-aplikacji)

---

## 1. Cel i Architektura Projektu

### Cel
Celem projektu jest stworzenie inteligentnego asystenta, ktÃ³ry potrafi precyzyjnie odpowiadaÄ‡ na pytania dotyczÄ…ce RozporzÄ…dzenia o Ochronie Danych Osobowych (RODO). Aplikacja ma byÄ‡ oparta wyÅ‚Ä…cznie na treÅ›ci oficjalnego dokumentu, aby zapewniÄ‡ wiernoÅ›Ä‡ i wiarygodnoÅ›Ä‡ odpowiedzi, eliminujÄ…c ryzyko "halucynacji" modelu AI.

### Architektura
Projekt opiera siÄ™ na nowoczesnej architekturze **RAG (Retrieval-Augmented Generation)**, ktÃ³ra skÅ‚ada siÄ™ z trzech gÅ‚Ã³wnych czÄ™Å›ci:
- **Backend API (FastAPI):** Serce aplikacji, ktÃ³re obsÅ‚uguje logikÄ™ i komunikuje siÄ™ z modelem AI.
- **Baza Wektorowa (ChromaDB):** Przechowuje przetworzonÄ… "wiedzÄ™" z dokumentu RODO w formie wektorÃ³w.
- **Frontend (Streamlit):** Prosty i interaktywny interfejs graficzny dla uÅ¼ytkownika koÅ„cowego.

---

## 2. Krok 1: Struktura Projektu i Konfiguracja Åšrodowiska

Na poczÄ…tku tworzymy logicznÄ… strukturÄ™ folderÃ³w i plikÃ³w konfiguracyjnych.

### Struktura folderÃ³w:
```
project_rodo_ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app_ui.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

### `requirements.txt`
Plik z listÄ… wszystkich niezbÄ™dnych bibliotek.
```python
fastapi
uvicorn[standard]
python-dotenv
langchain
langchain-openai
langchain-community
pypdf
chromadb
tiktoken
streamlit
requests
```
**Instalacja:** `pip install -r requirements.txt`

### `.env.example`
Szablon dla pliku `.env`, ktÃ³ry bÄ™dzie przechowywaÅ‚ nasz klucz API.
```
OPENAI_API_KEY="sk-..."
```
UÅ¼ytkownik musi skopiowaÄ‡ ten plik do `.env` i wkleiÄ‡ swÃ³j klucz.

### `.gitignore`
Plik informujÄ…cy Git, ktÃ³re pliki i foldery ma ignorowaÄ‡. To kluczowy element Å›wiadczÄ…cy o dobrych praktykach.
```
.venv/
.env
__pycache__/
vector_db/
```

---

## 3. Krok 2: Proces Ingestii Danych â€“ Tworzenie Bazy Wiedzy

Ten etap polega na przetworzeniu dokumentu PDF na format zrozumiaÅ‚y dla maszyny. Tworzymy dedykowany skrypt, ktÃ³ry uruchamiamy tylko raz.

### `app/ingest_data.py`
Skrypt ten wykonuje nastÄ™pujÄ…ce operacje:
1.  Wczytuje dokument PDF z folderu `data/`.
2.  Dzieli go na mniejsze, nakÅ‚adajÄ…ce siÄ™ na siebie fragmenty tekstu (chunki).
3.  UÅ¼ywa modelu embeddingÃ³w OpenAI do zamiany kaÅ¼dego fragmentu na wektor liczbowy.
4.  Zapisuje wektory wraz z oryginalnymi fragmentami tekstu w bazie wektorowej ChromaDB na dysku.

```python
# app/ingest_data.py

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
import os

load_dotenv()

PDF_PATH = os.path.join(os.path.dirname(__file__), '..', 'data', 'rodo_pl.pdf')
DB_PATH = os.path.join(os.path.dirname(__file__), '..', 'vector_db')

def main():
    print("Rozpoczynam proces ingestii danych...")

    if not os.path.exists(PDF_PATH):
        print(f"BÅÄ„D: Plik PDF nie zostaÅ‚ znaleziony pod Å›cieÅ¼kÄ…: {PDF_PATH}")
        return

    loader = PyPDFLoader(PDF_PATH)
    documents = loader.load()
    print(f"ZaÅ‚adowano {len(documents)} stron.")

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=120)
    chunks = text_splitter.split_documents(documents)
    print(f"Dokument podzielono na {len(chunks)} fragmentÃ³w.")

    print("Tworzenie embeddingÃ³w i zapisywanie w bazie ChromaDB...")
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=DB_PATH
    )
    print("Proces ingestii zakoÅ„czony pomyÅ›lnie!")

if __name__ == "__main__":
    main()
```
**Uruchomienie:** `python app/ingest_data.py`

---

## 4. Krok 3: RdzeÅ„ Logiki AI â€“ Budowa ÅaÅ„cucha Q&A

W tym kroku tworzymy serce naszej aplikacji â€“ mechanizm, ktÃ³ry potrafi inteligentnie odpowiadaÄ‡ na pytania.

### `app/core.py`
Ten plik zawiera funkcjÄ™ `get_qa_chain`, ktÃ³ra:
1.  Åaduje wczeÅ›niej utworzonÄ… bazÄ™ wektorowÄ… ChromaDB.
2.  Konfiguruje model jÄ™zykowy (np. GPT-3.5-Turbo).
3.  Definiuje **Prompt Template** â€“ kluczowy zestaw instrukcji dla modelu AI, nakazujÄ…cy mu odpowiadaÄ‡ tylko na podstawie dostarczonego kontekstu.
4.  Tworzy **Retrievera**, ktÃ³ry na podstawie pytania uÅ¼ytkownika wyszukuje w bazie wektorowej najbardziej relevantne fragmenty tekstu.
5.  ÅÄ…czy wszystkie te elementy w jeden `RetrievalQA` chain z biblioteki LangChain.

```python
# app/core.py

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from dotenv import load_dotenv
import os

load_dotenv()

DB_PATH = os.path.join(os.path.dirname(__file__), '..', 'vector_db')

PROMPT_TEMPLATE = """
JesteÅ› precyzyjnym i pomocnym asystentem AI, ktÃ³ry specjalizuje siÄ™ w RODO.
Twoim zadaniem jest odpowiedzieÄ‡ na pytanie uÅ¼ytkownika wyÅ‚Ä…cznie na podstawie dostarczonego poniÅ¼ej Kontekstu.
JeÅ›li w KontekÅ›cie nie ma wystarczajÄ…cych informacji, odpowiedz:
"Na podstawie dostarczonych fragmentÃ³w dokumentu RODO nie jestem w stanie udzieliÄ‡ odpowiedzi na to pytanie."
Nie prÃ³buj wymyÅ›laÄ‡ odpowiedzi. Odpowiadaj zawsze w jÄ™zyku polskim.

Kontekst:
{context}

Pytanie:
{question}

OdpowiedÅº:
"""

def get_qa_chain():
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.0)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context", "question"])
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    return qa_chain
```

---

## 5. Krok 4: Backend API w FastAPI

Teraz udostÄ™pnimy naszÄ… logikÄ™ AI Å›wiatu za pomocÄ… profesjonalnego API.

### `app/main.py`
Ten plik:
1.  Tworzy instancjÄ™ aplikacji FastAPI.
2.  Podczas startu serwera, jednorazowo inicjalizuje Å‚aÅ„cuch Q&A z `core.py`.
3.  Definiuje endpoint `POST /ask`, ktÃ³ry przyjmuje pytanie od uÅ¼ytkownika w formacie JSON.
4.  UÅ¼ywa Pydantic do walidacji danych wejÅ›ciowych i wyjÅ›ciowych, co zapewnia bezpieczeÅ„stwo i automatycznÄ… dokumentacjÄ™.
5.  Przekazuje pytanie do Å‚aÅ„cucha QA, otrzymuje odpowiedÅº i zwraca jÄ… do klienta wraz z dokumentami ÅºrÃ³dÅ‚owymi.

```python
# app/main.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from app.core import get_qa_chain
from typing import List

app = FastAPI(
    title="RODO Ekspert AI API",
    description="API do zadawania pytaÅ„ na temat RODO."
)

qa_chain = None

@app.on_event("startup")
def startup_event():
    global qa_chain
    qa_chain = get_qa_chain()

class QueryRequest(BaseModel):
    query: str

class DocumentMetadata(BaseModel):
    page: int
    source: str

class Document(BaseModel):
    page_content: str
    metadata: DocumentMetadata

class QueryResponse(BaseModel):
    answer: str
    source_documents: List[Document]

@app.post("/ask", response_model=QueryResponse)
def ask_question(request: QueryRequest):
    if not qa_chain:
        raise HTTPException(status_code=503, detail="Serwer nie jest gotowy.")
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="Pytanie nie moÅ¼e byÄ‡ puste.")
    
    try:
        result = qa_chain.invoke({"query": request.query})
        return {
            "answer": result.get("result", ""),
            "source_documents": result.get("source_documents", [])
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```
**Uruchomienie serwera API:** `uvicorn app.main:app --reload`

---

## 6. Krok 5: Interfejs UÅ¼ytkownika w Streamlit

Na koniec tworzymy prosty i intuicyjny interfejs graficzny.

### `ui/app_ui.py`
Ten skrypt:
1.  Tworzy interfejs webowy za pomocÄ… biblioteki Streamlit, przypominajÄ…cy okno czatu.
2.  ZarzÄ…dza historiÄ… konwersacji w stanie sesji.
3.  Gdy uÅ¼ytkownik wpisze pytanie, wysyÅ‚a zapytanie `POST` do naszego API w FastAPI.
4.  Odbiera odpowiedÅº z backendu i wyÅ›wietla jÄ… uÅ¼ytkownikowi.
5.  Dodatkowo, w rozwijanej sekcji (`expander`), pokazuje fragmenty dokumentu, na podstawie ktÃ³rych zostaÅ‚a wygenerowana odpowiedÅº, co zwiÄ™ksza transparentnoÅ›Ä‡.

```python
# ui/app_ui.py

import streamlit as st
import requests

API_URL = "http://127.0.0.1:8000/ask"

st.set_page_config(page_title="RODO Ekspert AI", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– RODO Ekspert AI")
st.caption("Zadaj pytanie dotyczÄ…ce RODO, a AI odpowie na podstawie oficjalnego tekstu rozporzÄ…dzenia.")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Jak brzmi Twoje pytanie?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("AnalizujÄ™ dokument... â³")
        
        try:
            response = requests.post(API_URL, json={"query": prompt}, timeout=120)
            response.raise_for_status()
            
            result = response.json()
            answer = result.get("answer", "WystÄ…piÅ‚ bÅ‚Ä…d.")
            message_placeholder.markdown(answer)
            
            with st.expander("Zobacz ÅºrÃ³dÅ‚a odpowiedzi"):
                for doc in result.get("source_documents", []):
                    st.info(f"**Å¹rÃ³dÅ‚o (strona {doc['metadata']['page']}):**\n\n{doc['page_content']}")

            st.session_state.messages.append({"role": "assistant", "content": answer})

        except requests.exceptions.RequestException as e:
            error_message = f"BÅ‚Ä…d poÅ‚Ä…czenia z API: {e}"
            message_placeholder.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})
```
**Uruchomienie interfejsu:** `streamlit run ui/app_ui.py`

---

## 7. Krok 6: Uruchomienie Kompletnej Aplikacji

Aby uruchomiÄ‡ caÅ‚y system, potrzebujesz dwÃ³ch otwartych okien terminala w gÅ‚Ã³wnym folderze projektu.

1.  **W pierwszym terminalu** uruchom serwer backendowy:
    ```bash
    uvicorn app.main:app --reload
    ```
2.  **W drugim terminalu** uruchom interfejs uÅ¼ytkownika:
    ```bash
    streamlit run ui/app_ui.py
    ```

Aplikacja otworzy siÄ™ automatycznie w przeglÄ…darce, gotowa do dziaÅ‚ania!

---

To wszystko! WÅ‚aÅ›nie przeszedÅ‚eÅ› przez caÅ‚y proces tworzenia zaawansowanej aplikacji AI. Gratulacje!
