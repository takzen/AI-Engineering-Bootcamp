# ModuÅ‚ 11, punkt 110: Praktyczna Realizacja Projektu KoÅ„cowego
# Projekt: Specjalistyczny Doradca AI (RODO Ekspert)
#
# Witaj w module finaÅ‚owym! To tutaj caÅ‚a zdobyta wiedza zÅ‚oÅ¼y siÄ™ w jeden, dziaÅ‚ajÄ…cy i imponujÄ…cy projekt.
# Zbudujemy od podstaw aplikacjÄ™, ktÃ³ra bÄ™dzie ekspertem w dziedzinie RODO.
# BÄ™dzie to Twoja wizytÃ³wka jako AI Engineera â€“ dowÃ³d, Å¼e potrafisz nie tylko korzystaÄ‡ z AI, ale tworzyÄ‡ realne, wartoÅ›ciowe narzÄ™dzia.
#
# Cel projektu: Stworzenie aplikacji webowej, ktÃ³ra precyzyjnie odpowiada na pytania dotyczÄ…ce
# RozporzÄ…dzenia o Ochronie Danych Osobowych (RODO), bazujÄ…c wyÅ‚Ä…cznie na jego treÅ›ci.
#
# Stack technologiczny, ktÃ³rego uÅ¼yjemy:
# - Backend API: FastAPI
# - Orkiestracja AI: LangChain
# - Baza Wiedzy: Mechanizm RAG (Retrieval-Augmented Generation) z bazÄ… wektorowÄ… ChromaDB
# - Frontend: Streamlit
#
# Zaczynajmy!

# ==============================================================================
# Krok 1: Przygotowanie Å›rodowiska i struktury projektu
# ==============================================================================
#
# Dobre przygotowanie to podstawa. Zadbajmy o porzÄ…dek od samego poczÄ…tku.

# 1. StwÃ³rz wirtualne Å›rodowisko
# W terminalu, w gÅ‚Ã³wnym folderze projektu, wykonaj:
# python -m venv .venv
# source .venv/bin/activate  # na Linux/macOS
# .venv\Scripts\activate      # na Windows

# 2. Zainstaluj niezbÄ™dne biblioteki
# StwÃ³rz plik 'requirements.txt' i wklej do niego poniÅ¼sze zaleÅ¼noÅ›ci.
# NastÄ™pnie zainstaluj je poleceniem: pip install -r requirements.txt
#
# Plik requirements.txt:
# langchain
# openai
# python-dotenv
# uvicorn
# fastapi
# pydantic
# pypdf
# chromadb
# tiktoken
# streamlit
# requests

# 3. StwÃ³rz strukturÄ™ folderÃ³w
# Nasz projekt bÄ™dzie wyglÄ…daÅ‚ tak:
#
# /twoj-projekt-rodo/
# |-- .venv/
# |-- app/
# |   |-- main.py             # GÅ‚Ã³wny plik z logikÄ… FastAPI
# |   |-- core.py             # Serce naszej aplikacji - logika RAG i LangChain
# |   |-- ingest_data.py      # Skrypt do przetwarzania dokumentu RODO i tworzenia bazy wektorowej
# |
# |-- data/
# |   |-- rodo_pl.pdf         # Tutaj umieÅ›Ä‡ plik PDF z tekstem RODO
# |
# |-- ui/
# |   |-- app_ui.py           # Plik z interfejsem uÅ¼ytkownika w Streamlit
# |
# |-- .env                    # Plik z kluczem API do OpenAI
# |-- requirements.txt

# ==============================================================================
# Krok 2: Pozyskanie i Przetworzenie Danych (Ingestion)
# ==============================================================================
#
# To jest serce mechanizmu RAG. Musimy "nauczyÄ‡" naszÄ… aplikacjÄ™ treÅ›ci RODO.
# Zrobimy to w pliku 'app/ingest_data.py'.

# 1. ZnajdÅº i pobierz dokument
# ZnajdÅº w internecie peÅ‚ny, jednolity tekst RODO w formacie PDF.
# Zapisz go w folderze 'data/' jako 'rodo_pl.pdf'.

# 2. Napisz skrypt 'ingest_data.py'
# Ten skrypt bÄ™dzie jednorazowo uruchamiany, aby przetworzyÄ‡ PDF i zapisaÄ‡ go w bazie wektorowej.

# przykÅ‚adowy kod dla app/ingest_data.py:
"""
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from dotenv import load_dotenv

load_dotenv()

# ÅšcieÅ¼ka do dokumentu i do bazy danych wektorowej
PDF_PATH = "../data/rodo_pl.pdf"
DB_PATH = "../vector_db"

def main():
    # 1. Åadowanie dokumentu PDF
    loader = PyPDFLoader(PDF_PATH)
    documents = loader.load()
    print(f"ZaÅ‚adowano {len(documents)} stron z dokumentu.")

    # 2. Dzielenie tekstu na mniejsze fragmenty (chunki)
    # To kluczowy krok, aby zmieÅ›ciÄ‡ fragmenty w kontekÅ›cie modelu i uzyskaÄ‡ trafne wyniki.
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    print(f"Podzielono dokument na {len(chunks)} fragmentÃ³w.")

    # 3. Tworzenie embeddingÃ³w i zapisywanie w bazie wektorowej ChromaDB
    # Embeddingi to numeryczne reprezentacje tekstu.
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=DB_PATH
    )
    print("Baza wektorowa zostaÅ‚a pomyÅ›lnie utworzona i zapisana.")

if __name__ == "__main__":
    main()
"""

# Uruchom ten skrypt z terminala, bÄ™dÄ…c w folderze 'app/':
# python ingest_data.py
# Po wykonaniu, w gÅ‚Ã³wnym folderze projektu powstanie nowy katalog 'vector_db/'.

# ==============================================================================
# Krok 3: Budowa Rdzenia Logiki AI (app/core.py)
# ==============================================================================
#
# Tutaj stworzymy logikÄ™, ktÃ³ra przyjmuje pytanie i generuje odpowiedÅº.
# UÅ¼yjemy do tego LangChain, aby poÅ‚Ä…czyÄ‡ LLM z naszÄ… bazÄ… wiedzy.

# przykÅ‚adowy kod dla app/core.py:
"""
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

load_dotenv()

DB_PATH = "../vector_db"

# Szablon promptu â€“ to tutaj dzieje siÄ™ magia!
# Nakazujemy modelowi, aby byÅ‚ precyzyjny, odpowiadaÅ‚ po polsku
# i bazowaÅ‚ WYÅÄ„CZNIE na dostarczonym kontekÅ›cie (fragmentach RODO).
PROMPT_TEMPLATE = \"\"\"
JesteÅ› precyzyjnym asystentem AI, ekspertem od rozporzÄ…dzenia RODO.
Odpowiadaj na pytania TYLKO I WYÅÄ„CZNIE na podstawie dostarczonego poniÅ¼ej kontekstu.
JeÅ›li w kontekÅ›cie nie ma odpowiedzi na pytanie, odpowiedz: "Na podstawie dostÄ™pnych danych nie jestem w stanie odpowiedzieÄ‡ na to pytanie.".
Odpowiadaj rzeczowo i konkretnie, w jÄ™zyku polskim.

Kontekst:
{context}

Pytanie:
{question}

OdpowiedÅº:
\"\"\"

def get_qa_chain():
    # 1. ZaÅ‚aduj istniejÄ…cÄ… bazÄ™ wektorowÄ…
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

    # 2. Skonfiguruj model LLM
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.0)

    # 3. StwÃ³rz retriever, ktÃ³ry bÄ™dzie wyszukiwaÅ‚ relevantne fragmenty w bazie
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    # 4. PoÅ‚Ä…cz wszystko w Å‚aÅ„cuch RetrievalQA
    prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context", "question"])
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    return qa_chain
"""

# ==============================================================================
# Krok 4: Uruchomienie API za pomocÄ… FastAPI (app/main.py)
# ==============================================================================
#
# Czas udostÄ™pniÄ‡ naszÄ… logikÄ™ Å›wiatu za pomocÄ… prostego API.

# przykÅ‚adowy kod dla app/main.py:
"""
from fastapi import FastAPI
from pydantic import BaseModel
from app.core import get_qa_chain

app = FastAPI(
    title="RODO Ekspert API",
    description="API do zadawania pytaÅ„ na temat RODO"
)

# Inicjalizacja Å‚aÅ„cucha QA przy starcie aplikacji
qa_chain = get_qa_chain()

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    answer: str
    sources: list[str]

@app.post("/ask", response_model=QueryResponse)
def ask_question(request: QueryRequest):
    \"\"\"
    Endpoint przyjmuje pytanie uÅ¼ytkownika i zwraca odpowiedÅº wygenerowanÄ…
    przez model oraz ÅºrÃ³dÅ‚a, na ktÃ³rych siÄ™ oparÅ‚.
    \"\"\"
    result = qa_chain({"query": request.query})
    
    answer = result.get("result", "WystÄ…piÅ‚ bÅ‚Ä…d.")
    source_documents = result.get("source_documents", [])
    
    sources = [doc.metadata.get("source", "") for doc in source_documents]
    
    return {"answer": answer, "sources": list(set(sources))}
"""

# Aby uruchomiÄ‡ serwer API, przejdÅº do gÅ‚Ã³wnego folderu projektu i wykonaj w terminalu:
# uvicorn app.main:app --reload

# MoÅ¼esz teraz przetestowaÄ‡ API, np. wbudowanej dokumentacji Swiggera pod adresem http://127.0.0.1:8000/docs

# ==============================================================================
# Krok 5: Stworzenie Interfejsu UÅ¼ytkownika (ui/app_ui.py)
# ==============================================================================
#
# Zbudujmy prosty, ale funkcjonalny interfejs, aby kaÅ¼dy mÃ³gÅ‚ skorzystaÄ‡ z naszej aplikacji.

# przykÅ‚adowy kod dla ui/app_ui.py:
"""
import streamlit as st
import requests

API_URL = "http://127.0.0.1:8000/ask"

st.set_page_config(page_title="RODO Ekspert AI", layout="centered")
st.title("ğŸ¤– RODO Ekspert AI")
st.caption("Zadaj pytanie dotyczÄ…ce RODO, a AI odpowie na podstawie oficjalnego dokumentu.")

# Inicjalizacja historii czatu
if "messages" not in st.session_state:
    st.session_state.messages = []

# WyÅ›wietlanie historii czatu
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Pole do wprowadzania pytania przez uÅ¼ytkownika
if prompt := st.chat_input("Jakie jest Twoje pytanie?"):
    # WyÅ›wietl pytanie uÅ¼ytkownika
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Przygotuj siÄ™ na odpowiedÅº AI
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("AnalizujÄ™ dokument RODO... â³")
        
        try:
            # WyÅ›lij zapytanie do API
            response = requests.post(API_URL, json={"query": prompt})
            response.raise_for_status() # SprawdÅº czy nie ma bÅ‚Ä™du HTTP
            
            result = response.json()
            answer = result.get("answer", "Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d.")
            
            message_placeholder.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})

        except requests.exceptions.RequestException as e:
            message_placeholder.error(f"BÅ‚Ä…d poÅ‚Ä…czenia z API: {e}")
            st.session_state.messages.append({"role": "assistant", "content": f"BÅ‚Ä…d: {e}"})

"""

# Aby uruchomiÄ‡ interfejs, otwÃ³rz nowy terminal i (w gÅ‚Ã³wnym folderze projektu) wykonaj:
# streamlit run ui/app_ui.py

# ==============================================================================
# Krok 6: Testowanie, Optymalizacja i Prezentacja w Portfolio
# ==============================================================================
#
# Masz dziaÅ‚ajÄ…cÄ… aplikacjÄ™! Teraz czas na pracÄ™ inÅ¼yniera.

# 1. Testowanie:
# - Zadawaj trudne, wieloznaczne pytania.
# - Sprawdzaj, czy model faktycznie odmawia odpowiedzi, gdy nie ma jej w kontekÅ›cie.
# - Testuj pytania o dane, ktÃ³rych nie ma w RODO (np. "Jaka jest stolica Polski?").

# 2. Optymalizacja:
# - Eksperymentuj z `chunk_size` i `chunk_overlap` w `ingest_data.py`. Czy mniejsze/wiÄ™ksze fragmenty dajÄ… lepsze wyniki?
# - Modyfikuj `PROMPT_TEMPLATE` w `core.py`. Dodaj instrukcje, aby model cytowaÅ‚ konkretne artykuÅ‚y.
# - ZmieÅ„ `search_kwargs={"k": 3}` w `core.py`. Czy pobranie wiÄ™kszej liczby fragmentÃ³w (`k=5`) poprawia jakoÅ›Ä‡ odpowiedzi?

# 3. Budowanie Portfolio (ModuÅ‚y 107-108):
# - StwÃ³rz wspaniaÅ‚y plik `README.md` w gÅ‚Ã³wnym folderze projektu. Opisz w nim cel, architekturÄ™ i jak uruchomiÄ‡ aplikacjÄ™.
# - Nagraj krÃ³tkie wideo (np. za pomocÄ… Loom) pokazujÄ…ce dziaÅ‚anie aplikacji.
# - UmieÅ›Ä‡ projekt na swoim GitHubie. To TwÃ³j najwaÅ¼niejszy zasÃ³b podczas szukania pracy.
# - Opisz napotkane wyzwania (np. "problem z halucynacjami modelu i jak go rozwiÄ…zaÅ‚em poprzez inÅ¼ynieriÄ™ promptÃ³w").

#
# Gratulacje! UkoÅ„czyÅ‚eÅ› swÃ³j pierwszy kompleksowy projekt AI.
# To dowÃ³d Twoich umiejÄ™tnoÅ›ci i fantastyczny start do dalszego rozwoju w tej ekscytujÄ…cej dziedzinie.
#