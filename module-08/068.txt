ModuÅ‚ 8, Punkt 68: Analiza bÅ‚Ä™dÃ³w i debugowanie aplikacji

W poprzedniej lekcji nauczyliÅ›my siÄ™, jak monitorowaÄ‡ naszÄ… aplikacjÄ™ i zbieraÄ‡ dane za pomocÄ… LangSmith. Zbieranie danych to jedno, ale prawdziwa wartoÅ›Ä‡ pojawia siÄ™, 
gdy zaczynamy ich uÅ¼ywaÄ‡ do znajdowania i naprawiania bÅ‚Ä™dÃ³w.

Debugowanie aplikacji opartych o LLM jest wyjÄ…tkowo trudne, poniewaÅ¼ problemy rzadko sÄ… prostymi bÅ‚Ä™dami skÅ‚adni. Znacznie czÄ™Å›ciej sÄ… to subtelne problemy z logikÄ… promptu, 
nieoczekiwane zachowanie modelu lub bÅ‚Ä™dy w interakcji z narzÄ™dziami. LangSmith zostaÅ‚ stworzony, aby rzuciÄ‡ Å›wiatÅ‚o na te "szare strefy".

    Dwa rodzaje bÅ‚Ä™dÃ³w w aplikacjach AI

W LangSmith napotkamy na dwa gÅ‚Ã³wne typy problemÃ³w:

    Twarde bÅ‚Ä™dy (Hard Errors): To bÅ‚Ä™dy, ktÃ³re powodujÄ… rzucenie wyjÄ…tku i przerwanie dziaÅ‚ania aplikacji. PrzykÅ‚ady:

        BÅ‚Ä…d API (np. zÅ‚y klucz, przeciÄ…Å¼enie serwerÃ³w OpenAI).

        BÅ‚Ä…d wykonania narzÄ™dzia (np. narzÄ™dzie prÃ³bowaÅ‚o podzieliÄ‡ przez zero).

        BÅ‚Ä…d parsowania (model zwrÃ³ciÅ‚ niepoprawny JSON).
        W LangSmith takie Å›lady sÄ… wyraÅºnie oznaczone na czerwono.

    MiÄ™kkie bÅ‚Ä™dy (Soft Errors) / Problemy z jakoÅ›ciÄ…: To znacznie trudniejsza kategoria. Aplikacja technicznie dziaÅ‚a i zwraca odpowiedÅº, ale odpowiedÅº jest zÅ‚a. PrzykÅ‚ady:

        Halucynacje (model "wymyÅ›la" fakty).

        OdpowiedÅº nie na temat.

        OdpowiedÅº niekompletna lub zbyt ogÃ³lna.

        Nieuprzejmy lub nieodpowiedni ton.
        LangSmith sam z siebie nie wie, Å¼e to bÅ‚Ä…d. Tutaj kluczowe staje siÄ™ zbieranie opinii (feedback) od uÅ¼ytkownikÃ³w lub wewnÄ™trznych testerÃ³w.

    Workflow debugowania twardych bÅ‚Ä™dÃ³w w LangSmith

ZaÅ‚Ã³Å¼my, Å¼e w panelu projektu widzisz, Å¼e wskaÅºnik bÅ‚Ä™dÃ³w wzrÃ³sÅ‚ do 5%. Czas dziaÅ‚aÄ‡!

    Filtrowanie po bÅ‚Ä™dach: W tabeli Å›ladÃ³w uÅ¼yj filtra, aby wyÅ›wietliÄ‡ tylko te uruchomienia, ktÃ³re zakoÅ„czyÅ‚y siÄ™ bÅ‚Ä™dem (Status: Error).

    WybÃ³r Å›ladu do analizy: Kliknij na jeden z czerwonych Å›ladÃ³w, aby go otworzyÄ‡.

    Identyfikacja ÅºrÃ³dÅ‚a bÅ‚Ä™du: W widoku Å›ladu, przewiÅ„ hierarchiÄ™ operacji. Krok, ktÃ³ry zawiÃ³dÅ‚, bÄ™dzie rÃ³wnieÅ¼ oznaczony na czerwono.

    Analiza szczegÃ³Å‚Ã³w kroku: Kliknij na ten czerwony krok. Po prawej stronie, w panelu szczegÃ³Å‚Ã³w, znajdziesz:

        Inputs: DokÅ‚adne dane wejÅ›ciowe, ktÃ³re otrzymaÅ‚ ten krok. To kluczowe â€“ moÅ¼esz zobaczyÄ‡, jakie dane doprowadziÅ‚y do bÅ‚Ä™du.

        Error: PeÅ‚ny komunikat o bÅ‚Ä™dzie (traceback), dokÅ‚adnie taki, jakbyÅ› go widziaÅ‚ w konsoli Pythona.

    Reprodukcja i naprawa:

        Teraz, gdy znasz dokÅ‚adne dane wejÅ›ciowe i komunikat o bÅ‚Ä™dzie, moÅ¼esz sprÃ³bowaÄ‡ odtworzyÄ‡ ten bÅ‚Ä…d lokalnie w swoim Å›rodowisku deweloperskim.

        JeÅ›li problemem byÅ‚ np. zÅ‚y format danych przekazywanych do narzÄ™dzia, moÅ¼esz poprawiÄ‡ logikÄ™ agenta lub prompt, aby generowaÅ‚ poprawne dane.

    Workflow debugowania miÄ™kkich bÅ‚Ä™dÃ³w (problemy z jakoÅ›ciÄ…)

ZaÅ‚Ã³Å¼my, Å¼e uÅ¼ytkownicy zgÅ‚aszajÄ…, Å¼e chatbot czÄ™sto daje nieprecyzyjne odpowiedzi.

    Wykorzystanie opinii (Feedback):

        W interfejsie swojej aplikacji zaimplementuj prosty mechanizm oceny (np. ğŸ‘/ğŸ‘). Te oceny wysyÅ‚aj do LangSmith i przypisuj do odpowiednich Å›ladÃ³w.

        W panelu LangSmith moÅ¼esz teraz filtrowaÄ‡ Å›lady, ktÃ³re otrzymaÅ‚y negatywnÄ… opiniÄ™ (np. "ocena = 0").

    Analiza "zÅ‚ego" Å›ladu:

        OtwÃ³rz Å›lad z negatywnÄ… ocenÄ…. Przeczytaj caÅ‚Ä… konwersacjÄ™.

        Zanurkuj w szczegÃ³Å‚y kroku, ktÃ³ry wygenerowaÅ‚ finalnÄ… odpowiedÅº (zwykle ChatOpenAI).

        SprawdÅº ostateczny prompt! Przeanalizuj, jaki dokÅ‚adnie prompt (z caÅ‚Ä… historiÄ… i kontekstem) otrzymaÅ‚ model. Czy byÅ‚ on jasny? Czy zawieraÅ‚ wszystkie niezbÄ™dne 
        informacje? Bardzo czÄ™sto problem leÅ¼y wÅ‚aÅ›nie tutaj.

    Tworzenie zestawu testowego:

        Gdy znajdziesz przykÅ‚ad zÅ‚ej odpowiedzi, nie pozwÃ³l mu uciec! W LangSmith moÅ¼esz jednym klikniÄ™ciem dodaÄ‡ ten Å›lad do zestawu danych (dataset).

        Z czasem budujesz w ten sposÃ³b "bibliotekÄ™" trudnych przypadkÃ³w i przykÅ‚adÃ³w bÅ‚Ä™dÃ³w.

    Eksperymentowanie i regresja:

        WrÃ³Ä‡ do swojego kodu (lub przepÅ‚ywu w LangFlow) i sprÃ³buj poprawiÄ‡ prompt lub logikÄ™.

        NastÄ™pnie, moÅ¼esz uruchomiÄ‡ swÃ³j zestaw testowy na nowej wersji aplikacji, aby sprawdziÄ‡, czy:
        a) NaprawiÅ‚eÅ› stary bÅ‚Ä…d.
        b) Nie zepsuÅ‚eÅ› przy okazji czegoÅ›, co wczeÅ›niej dziaÅ‚aÅ‚o (test regresji).

    "Playground" â€“ interaktywne debugowanie:
    LangSmith oferuje funkcjÄ™ "Playground". MoÅ¼esz otworzyÄ‡ dowolny krok z przeszÅ‚oÅ›ci, zmodyfikowaÄ‡ jego dane wejÅ›ciowe (np. poprawiÄ‡ fragment promptu) i uruchomiÄ‡ go 
    ponownie bezpoÅ›rednio w interfejsie LangSmith, aby zobaczyÄ‡, czy zmiana przyniosÅ‚a oczekiwany efekt. To niezwykle przyspiesza proces iteracji nad promptami.

    Podsumowanie

LangSmith przeksztaÅ‚ca debugowanie z reaktywnego gaszenia poÅ¼arÃ³w w proaktywny, ustrukturyzowany proces.

NajwaÅ¼niejsze do zapamiÄ™tania:

    RozrÃ³Å¼niaj twarde i miÄ™kkie bÅ‚Ä™dy: Twarde bÅ‚Ä™dy sÄ… Å‚atwiejsze do zdiagnozowania, miÄ™kkie wymagajÄ… oceny i opinii.

    Åšlad to Twoje miejsce zbrodni: Analizuj szczegÃ³Å‚y Å›ladÃ³w, aby zidentyfikowaÄ‡ dokÅ‚adny krok i dane wejÅ›ciowe, ktÃ³re spowodowaÅ‚y problem.

    Prompt jest najczÄ™stszym winowajcÄ…: W przypadku problemÃ³w z jakoÅ›ciÄ…, zawsze zaczynaj od analizy ostatecznego promptu, ktÃ³ry trafiÅ‚ do LLM.

    Buduj bibliotekÄ™ testÃ³w: KaÅ¼dy znaleziony bÅ‚Ä…d to cenna lekcja. Dodawaj problematyczne przypadki do zestawÃ³w danych, aby testowaÄ‡ przyszÅ‚e wersje swojej aplikacji.

    Iteruj w Playground: UÅ¼ywaj Playground do szybkiego eksperymentowania ze zmianami w promptach bez koniecznoÅ›ci modyfikacji i wdraÅ¼ania caÅ‚ego kodu.

DziÄ™ki tym technikom, proces poprawy Twojej aplikacji staje siÄ™ systematyczny, mierzalny i znacznie bardziej efektywny.